{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "\n",
    "url = \"https://www.rbc.ru/search/?project=rbcnews&query=коронавирус\"\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) \n",
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "coverpage_news = []\n",
    "for i in range(3): #scroll 3000 times\n",
    "    ActionChains(driver).send_keys(Keys.END).perform()\n",
    "soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "for element in soup.find_all('a', attrs={'class': 'search-item__link'}):\n",
    "    coverpage_news.append(element)\n",
    "\n",
    "\n",
    "news_contents = []\n",
    "list_links = []\n",
    "\n",
    "for i in np.arange(0, len(coverpage_news)):\n",
    "    # Getting the link of the article\n",
    "    link = \"https://\" + coverpage_news[i]['href'].strip(\"https://\") \n",
    "    list_links.append(link)\n",
    "    # Reading the content (it is divided in paragraphs)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content)\n",
    "    body = soup_article.find_all('div', class_='article__text article__text_free')\n",
    "    x = body[0].find_all('p')\n",
    "    \n",
    "    # Unifying the paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    news_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymystem3\n",
    "import stop_words\n",
    "\n",
    "df = pd.read_excel('ria_raw.xlsx', engine=\"openpyxl\", index_col = 0)\n",
    "\n",
    "df['news_contents'] = df['news_contents'].str.lower()\n",
    "def delete_punctuation(text):\n",
    "    clear_text = ''\n",
    "    for symbol in text:\n",
    "        if symbol.isalpha():\n",
    "            clear_text += symbol\n",
    "        else:\n",
    "            clear_text += ' '\n",
    "    return clear_text\n",
    "\n",
    "def delete_double(text):\n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ', ' ').strip()\n",
    "    return text\n",
    "def makestr(text):\n",
    "    text = str(text)\n",
    "    return text\n",
    "    \n",
    "df['news_contents'] = df['news_contents'].apply(makestr)\n",
    "df['news_contents'] = df['news_contents'].apply(delete_punctuation)\n",
    "df['news_contents'] = df['news_contents'].apply(delete_double)\n",
    "\n",
    "mstem = pymystem3.Mystem()\n",
    "\n",
    "def lemmatize(text):\n",
    "    return ''.join(mstem.lemmatize(text)).strip()\n",
    "    \n",
    "df['news_contents'] = df['news_contents'].apply(lemmatize)\n",
    "\n",
    "rus = stop_words.get_stop_words('russian')\n",
    "en = stop_words.get_stop_words('english')\n",
    "all_sw = rus + en\n",
    "len(all_sw)\n",
    "\n",
    "def delete_stop_words(text):\n",
    "    text = text.split()\n",
    "    clear_text = []\n",
    "    for word in text:\n",
    "        if word not in all_sw:\n",
    "            clear_text.append(word)\n",
    "    return ' '.join(clear_text)\n",
    "\n",
    "df['news_contents'] = df['news_contents'].apply(delete_stop_words)\n",
    "\n",
    "def delete_eng(text):\n",
    "    text = text.split()\n",
    "    clear_text = []\n",
    "    for word in text:\n",
    "        if ord(word[0]) > 1039:\n",
    "            clear_text.append(word)\n",
    "    return ' '.join(clear_text)\n",
    "    \n",
    "df['news_contents'] = df['news_contents'].apply(delete_eng)\n",
    "df.to_excel('ria_lemmatized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "df = pd.read_excel('rbk_lemmatized.xlsx', engine=\"openpyxl\", index_col = 0)\n",
    "np.random.seed(42)\n",
    "nltk.download('wordnet')\n",
    "\n",
    "doc_sample = df['news_contents'].iloc[0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    df['news_contents'].iloc[i] = df['news_contents'].iloc[i].split(' ')\n",
    "\n",
    "processed_docs = np.array(df['news_contents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary from ‘processed docs’ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out tokens that appear in less than 15 documents (absolute number) or more than 0.5 documents (fraction of total corpus size). After that, keep only the first 100000 most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document we create a dictionary reporting how many words and how many times those words appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "doc_4310 = corpus[4310]\n",
    "for i in range(len(doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(doc_4310[i][0], \n",
    "                                               dictionary[doc_4310[i][0]], \n",
    "doc_4310[i][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use Multicore to parallelize and speed up model training. Alpha and beta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num of topics prior (we’ll use default for the base model). Passes - analogue of epochs/iterations. Chunksize controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus, num_topics=10, \n",
    "                                       id2word = dictionary,\n",
    "                                       workers = 2, passes=10,\n",
    "                                       random_state=100,\n",
    "                                       chunksize=100)\n",
    "                                       \n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s calculate the baseline coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs,\n",
    "                                     dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tune the following hyperparameters:\n",
    "\n",
    "Number of Topics (K)\n",
    "\n",
    "Dirichlet hyperparameter alpha: Document-Topic Density\n",
    "\n",
    "Dirichlet hyperparameter beta: Word-Topic Density\n",
    "\n",
    "I'll use c_v as model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence(corpus, dictionary, k, a, b):\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,id2word=dictionary,\n",
    "                                           num_topics=k, random_state=100,\n",
    "                                           chunksize=100,passes=10,\n",
    "                                           alpha=a, eta=b)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs,\n",
    "                                     dictionary=dictionary, coherence='c_v')\n",
    "    return coherence_model_lda.get_coherence()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s call the function, and iterate it over the range of topics, alphas, and betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm #to have a progress bar\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "# Topics range\n",
    "min_topics, max_topics, step_size = 2, 11, 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.75), \n",
    "               gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.5)), \n",
    "               corpus]\n",
    "corpus_title = ['50% Corpus', '100% Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': [] }\n",
    "                 \n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=100)\n",
    "    \n",
    "    for i in range(len(corpus_sets)):\n",
    "        for k in topics_range:\n",
    "            for a in alpha:\n",
    "                for b in beta:\n",
    "                    cv = compute_coherence(corpus=corpus_sets[i],\n",
    "                                                  dictionary=dictionary, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_results_rbk.csv', index=False)\n",
    "    pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
